{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: This is the title of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:  \n",
    "Arin Rahim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import cv2 \n",
    "import mediapy as media\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import PIL\n",
    "import absl.logging \n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from official.projects.movinet.modeling import movinet\n",
    "#from official.projects.movinet.modeling import movinet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pycocotools.coco import COCO\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "mpl.rcParams.update({\n",
    "    'font.size': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arinrahim/Desktop/SKOLAN/ML/Assignments/Final assignment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two defined functions are used to preprocess the frame and extract the keypoints from it. Beacuse of a unknown reason I was not able to define these function in the utility file , so they were copied here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = tf.image.resize_with_pad(image, 192, 192)\n",
    "    image = tf.cast(image, tf.int32) \n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_keypoints_from_movenet(image, movenet):\n",
    "    input_image = preprocess_image(image)\n",
    "    outputs = movenet(input_image)\n",
    "    keypoints = outputs['output_0']\n",
    "    return keypoints.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go through the data correctly, os-functions were used to find the MP4 videos of each activity and loop through them. OpenCV was utillized to extract frames from the video and  Movenet was then used for pose estimation on these videos, specifically the lighting variant. This cell took a while to run and to avoid running it again , I have provide you with the data in the folder processes/pre_processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keypoint names\n",
    "keypoint_names = [\n",
    "    \"nose\", \"left_eye\",\n",
    "    \"right_eye\", \"left_ear\",\n",
    "    \"right_ear\", \"left_shoulder\",\n",
    "    \"right_shoulder\", \"left_elbow\", \n",
    "    \"right_elbow\", \"left_wrist\",\n",
    "    \"right_wrist\", \"left_hip\",\n",
    "    \"right_hip\", \"left_knee\", \n",
    "    \"right_knee\", \"left_ankle\",\n",
    "    \"right_ankle\"\n",
    "]\n",
    "\n",
    "# Define the activites number\n",
    "activity_to_number = {\n",
    "    'pushup' : 1, \n",
    "    'curl' : 2,\n",
    "    'fly' : 3,\n",
    "    'squat' : 4,\n",
    "    'birddog' : 5, \n",
    "    'superman' : 6,\n",
    "    'bicyclecrunch' : 7,\n",
    "    'legraise' : 8, \n",
    "    'armraise' : 9,\n",
    "    'overheadpress' : 10\n",
    "    }\n",
    "\n",
    "# Load model \n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "movenet = model.signatures['serving_default']\n",
    "  \n",
    " # List created to store the keypoints data for each frame of video\n",
    "data = []\n",
    "video_number = 0\n",
    "\n",
    "# Base directory where the folders are located\n",
    "base_dir = 'raw_data'\n",
    "\n",
    "\n",
    "# Loop through each main folder in the base directory\n",
    "for main_folder in os.listdir(base_dir):\n",
    "    # Construct the path to the first-level exercise folder\n",
    "    first_level_path = os.path.join(base_dir, main_folder)\n",
    "    \n",
    "    # Check if it's a directory to avoid processing any files directly under base_dir\n",
    "    if os.path.isdir(first_level_path):\n",
    "        \n",
    "        # Get the list of subdirectories (should be just one per your structure)\n",
    "        subdirectories = [d for d in os.listdir(first_level_path) if os.path.isdir(os.path.join(first_level_path, d))]\n",
    "        \n",
    "        # Check if there's at least one subdirectory and proceed\n",
    "        if subdirectories:\n",
    "            # Path to the actual folder containing MP4 files (second-level folder)\n",
    "            video_folder_path = os.path.join(first_level_path, subdirectories[0])\n",
    "            \n",
    "            # Loop through each MP4 file in the video folder\n",
    "            for video_file in os.listdir(video_folder_path):\n",
    "                video_path = os.path.join(video_folder_path, video_file)\n",
    "                \n",
    "                # Check if the file is an MP4 video\n",
    "                if os.path.isfile(video_path) and video_file.endswith('.mp4'):\n",
    "                    print(f\"Processing {video_file} in {main_folder}...\")\n",
    "                    video_number = video_number + 1\n",
    "                    print(\"Video number: \", video_number)\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    frame_number = 0\n",
    "                    \n",
    "                    while cap.isOpened():\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        \n",
    "                        frame_number += 1\n",
    "                        #print(\"Processing: frame: \", frame_number)\n",
    "                        keypoints = extract_keypoints_from_movenet(frame, movenet)\n",
    "                        \n",
    "                        #keypoints_flat = keypoints.flatten().tolist()\n",
    "                        activity_name = subdirectories[0]\n",
    "                        activity_label = activity_to_number[activity_name]\n",
    "                        record = {'Video' : f'{video_file}', 'activity_label': activity_label, 'frame_number': frame_number}\n",
    "\n",
    "                        # Append data including frame number, exercise label, and keypoints\n",
    "                        for i, kp in enumerate(keypoints[0, 0, :, :]): \n",
    "                            body_part = keypoint_names[i]\n",
    "                            x, y, confidence = kp[1], kp[0], kp[2] \n",
    "                            \n",
    "                            record[f'k_pts_{body_part}_x'] = x\n",
    "                            record[f'k_pts_{body_part}_y'] = y\n",
    "                            record[f'k_pts_{body_part}_confidence'] = confidence\n",
    "                        #print(record)\n",
    "                        data.append(record)\n",
    "                        #print(data)\n",
    "                        \n",
    "                            \n",
    "                    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should load the raw dataset for the task.  \n",
    "Remember to use relative paths to load any files in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the keypoint data in a dataframe and saves it to a CSV file. The file is saved in processes/preprocess_data \n",
    "#df = pd.DataFrame(data)\n",
    "#df_activity = df.copy()\n",
    "\n",
    "# Read the file directly to avoid re-running previous step\n",
    "df = pd.read_csv('processes/preprocess_data/PoseEstimationData.csv')\n",
    "df_activity = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>activity_label</th>\n",
       "      <th>frame_number</th>\n",
       "      <th>k_pts_nose_x</th>\n",
       "      <th>k_pts_nose_y</th>\n",
       "      <th>k_pts_nose_confidence</th>\n",
       "      <th>k_pts_left_eye_x</th>\n",
       "      <th>k_pts_left_eye_y</th>\n",
       "      <th>k_pts_left_eye_confidence</th>\n",
       "      <th>k_pts_right_eye_x</th>\n",
       "      <th>...</th>\n",
       "      <th>k_pts_left_knee_confidence</th>\n",
       "      <th>k_pts_right_knee_x</th>\n",
       "      <th>k_pts_right_knee_y</th>\n",
       "      <th>k_pts_right_knee_confidence</th>\n",
       "      <th>k_pts_left_ankle_x</th>\n",
       "      <th>k_pts_left_ankle_y</th>\n",
       "      <th>k_pts_left_ankle_confidence</th>\n",
       "      <th>k_pts_right_ankle_x</th>\n",
       "      <th>k_pts_right_ankle_y</th>\n",
       "      <th>k_pts_right_ankle_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000038.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375629</td>\n",
       "      <td>0.329855</td>\n",
       "      <td>0.575082</td>\n",
       "      <td>0.379877</td>\n",
       "      <td>0.318692</td>\n",
       "      <td>0.671976</td>\n",
       "      <td>0.358221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725594</td>\n",
       "      <td>0.282331</td>\n",
       "      <td>0.859699</td>\n",
       "      <td>0.834232</td>\n",
       "      <td>0.346216</td>\n",
       "      <td>0.977464</td>\n",
       "      <td>0.585235</td>\n",
       "      <td>0.245951</td>\n",
       "      <td>0.980421</td>\n",
       "      <td>0.492093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000038.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.374021</td>\n",
       "      <td>0.329474</td>\n",
       "      <td>0.556815</td>\n",
       "      <td>0.378901</td>\n",
       "      <td>0.318824</td>\n",
       "      <td>0.653531</td>\n",
       "      <td>0.356590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736575</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>0.862176</td>\n",
       "      <td>0.834284</td>\n",
       "      <td>0.346497</td>\n",
       "      <td>0.976944</td>\n",
       "      <td>0.531707</td>\n",
       "      <td>0.245042</td>\n",
       "      <td>0.980027</td>\n",
       "      <td>0.474869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000038.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.378061</td>\n",
       "      <td>0.328498</td>\n",
       "      <td>0.627583</td>\n",
       "      <td>0.379186</td>\n",
       "      <td>0.318344</td>\n",
       "      <td>0.656229</td>\n",
       "      <td>0.361404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738356</td>\n",
       "      <td>0.281732</td>\n",
       "      <td>0.861353</td>\n",
       "      <td>0.847008</td>\n",
       "      <td>0.346142</td>\n",
       "      <td>0.977287</td>\n",
       "      <td>0.577636</td>\n",
       "      <td>0.246038</td>\n",
       "      <td>0.979141</td>\n",
       "      <td>0.484306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000038.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.378496</td>\n",
       "      <td>0.326863</td>\n",
       "      <td>0.658308</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.316471</td>\n",
       "      <td>0.618782</td>\n",
       "      <td>0.361762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712585</td>\n",
       "      <td>0.282561</td>\n",
       "      <td>0.862615</td>\n",
       "      <td>0.841878</td>\n",
       "      <td>0.346289</td>\n",
       "      <td>0.977373</td>\n",
       "      <td>0.515678</td>\n",
       "      <td>0.245476</td>\n",
       "      <td>0.980479</td>\n",
       "      <td>0.477897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000038.mp4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.377487</td>\n",
       "      <td>0.327366</td>\n",
       "      <td>0.627432</td>\n",
       "      <td>0.378026</td>\n",
       "      <td>0.317184</td>\n",
       "      <td>0.572762</td>\n",
       "      <td>0.362475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721487</td>\n",
       "      <td>0.278926</td>\n",
       "      <td>0.864761</td>\n",
       "      <td>0.838640</td>\n",
       "      <td>0.346617</td>\n",
       "      <td>0.977318</td>\n",
       "      <td>0.554312</td>\n",
       "      <td>0.245807</td>\n",
       "      <td>0.980555</td>\n",
       "      <td>0.483587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Video  activity_label  frame_number  k_pts_nose_x  k_pts_nose_y  \\\n",
       "0  000038.mp4               2             1      0.375629      0.329855   \n",
       "1  000038.mp4               2             2      0.374021      0.329474   \n",
       "2  000038.mp4               2             3      0.378061      0.328498   \n",
       "3  000038.mp4               2             4      0.378496      0.326863   \n",
       "4  000038.mp4               2             5      0.377487      0.327366   \n",
       "\n",
       "   k_pts_nose_confidence  k_pts_left_eye_x  k_pts_left_eye_y  \\\n",
       "0               0.575082          0.379877          0.318692   \n",
       "1               0.556815          0.378901          0.318824   \n",
       "2               0.627583          0.379186          0.318344   \n",
       "3               0.658308          0.379104          0.316471   \n",
       "4               0.627432          0.378026          0.317184   \n",
       "\n",
       "   k_pts_left_eye_confidence  k_pts_right_eye_x  ...  \\\n",
       "0                   0.671976           0.358221  ...   \n",
       "1                   0.653531           0.356590  ...   \n",
       "2                   0.656229           0.361404  ...   \n",
       "3                   0.618782           0.361762  ...   \n",
       "4                   0.572762           0.362475  ...   \n",
       "\n",
       "   k_pts_left_knee_confidence  k_pts_right_knee_x  k_pts_right_knee_y  \\\n",
       "0                    0.725594            0.282331            0.859699   \n",
       "1                    0.736575            0.280325            0.862176   \n",
       "2                    0.738356            0.281732            0.861353   \n",
       "3                    0.712585            0.282561            0.862615   \n",
       "4                    0.721487            0.278926            0.864761   \n",
       "\n",
       "   k_pts_right_knee_confidence  k_pts_left_ankle_x  k_pts_left_ankle_y  \\\n",
       "0                     0.834232            0.346216            0.977464   \n",
       "1                     0.834284            0.346497            0.976944   \n",
       "2                     0.847008            0.346142            0.977287   \n",
       "3                     0.841878            0.346289            0.977373   \n",
       "4                     0.838640            0.346617            0.977318   \n",
       "\n",
       "   k_pts_left_ankle_confidence  k_pts_right_ankle_x  k_pts_right_ankle_y  \\\n",
       "0                     0.585235             0.245951             0.980421   \n",
       "1                     0.531707             0.245042             0.980027   \n",
       "2                     0.577636             0.246038             0.979141   \n",
       "3                     0.515678             0.245476             0.980479   \n",
       "4                     0.554312             0.245807             0.980555   \n",
       "\n",
       "   k_pts_right_ankle_confidence  \n",
       "0                      0.492093  \n",
       "1                      0.474869  \n",
       "2                      0.484306  \n",
       "3                      0.477897  \n",
       "4                      0.483587  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the first 5 rows of the dataframe\n",
    "df_activity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Train model with pose estimation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that was choosen to train and use for predicting the different activities was the SVM-model. The choice of model was based on errors that hindered me from using a neural network such as LSTM, and since time was short I wanted something simple and robust, therefore SVM seemed like an appropriate choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the split data \n",
    "X = df_activity.drop(['activity_label', 'Video', 'frame_number'], axis=1)\n",
    "y = df_activity['activity_label']\n",
    "\n",
    "# Split the dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()), (&#x27;svc&#x27;, SVC(C=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()), (&#x27;svc&#x27;, SVC(C=10))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=10)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(C=10))])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the svm model, and use the pipeline to scale the data\n",
    "svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=10))\n",
    "\n",
    "# Train the model using the training set\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91     10221\n",
      "           2       0.98      0.99      0.98     10534\n",
      "           3       0.98      0.98      0.98      6565\n",
      "           4       0.98      0.97      0.97     11793\n",
      "           5       0.94      0.96      0.95      8710\n",
      "           6       0.89      0.94      0.91     12815\n",
      "           7       0.88      0.88      0.88      6971\n",
      "           8       0.90      0.85      0.87     10263\n",
      "           9       0.97      0.98      0.98     13402\n",
      "          10       0.98      0.99      0.98     11301\n",
      "\n",
      "    accuracy                           0.94    102575\n",
      "   macro avg       0.94      0.94      0.94    102575\n",
      "weighted avg       0.94      0.94      0.94    102575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed performance report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task 1, a total of 51 features was used by the model including the keypoints of each joints left and right side, and the confidence score. For task 2, feature engineering was applied to extract more information from these raw data. Distance between the keypoints, distance ratio, and some joint angles were caluclated by using three functions. As I mentioned previously, the utility file did not work so the functions for calculating distance and angle are in the main script. \n",
    "\n",
    "Same reason here, I got errors when trying to use a LSTM-model so SVM was utilized again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the distance between two keypoints\n",
    "def distance_keypoints(x1, y1, x2, y2):\n",
    "    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "# Function for calculating the angle between three keypoints\n",
    "def angle_keypoints(ax, ay, bx, by, cx, cy):\n",
    "    ba = [ax - bx, ay - by]\n",
    "    bc = [cx - bx, cy - by]\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    return np.degrees(angle)\n",
    "    \n",
    "# Create a copy of the original data frame\n",
    "df_activity_features = df.copy()\n",
    "\n",
    "# An empty list to store the calculated features\n",
    "features = []\n",
    "\n",
    "for _, row in df_activity_features.iterrows():\n",
    "    \n",
    "    # Calculate distance between different joints for left and right side \n",
    "    \n",
    "    distance_wrist_shoulder_left = distance_keypoints(row['k_pts_left_wrist_x'], row['k_pts_left_wrist_y'],\n",
    "                                                 row['k_pts_left_shoulder_x'], row['k_pts_left_shoulder_y'])\n",
    "    \n",
    "    distance_wrist_shoulder_right = distance_keypoints(row['k_pts_right_wrist_x'], row['k_pts_right_wrist_y'],\n",
    "                                                 row['k_pts_right_shoulder_x'], row['k_pts_right_shoulder_y'])\n",
    "    \n",
    "    distance_hip_ankle_left = distance_keypoints(row['k_pts_left_hip_x'], row['k_pts_left_hip_y'],\n",
    "                                            row['k_pts_left_ankle_x'], row['k_pts_left_ankle_y'])\n",
    "    \n",
    "    distance_hip_ankle_right = distance_keypoints(row['k_pts_right_hip_x'], row['k_pts_right_hip_y'],\n",
    "                                            row['k_pts_right_ankle_x'], row['k_pts_right_ankle_y'])\n",
    "    \n",
    "    distance_nose_ankle_left = distance_keypoints(row['k_pts_nose_x'], row['k_pts_nose_y'],\n",
    "                                                row['k_pts_left_ankle_x'], row['k_pts_left_ankle_y'])\n",
    "    \n",
    "    distance_nose_ankle_right = distance_keypoints(row['k_pts_nose_x'], row['k_pts_nose_y'],\n",
    "                                                   row['k_pts_right_ankle_x'], row['k_pts_right_ankle_y'])\n",
    "    \n",
    "    distance_nose_knee_left = distance_keypoints(row['k_pts_nose_x'], row['k_pts_nose_y'], \n",
    "                                                 row['k_pts_left_knee_x'], row['k_pts_left_knee_y'])\n",
    "    \n",
    "    distance_nose_knee_right = distance_keypoints(row['k_pts_nose_x'], row['k_pts_nose_y'], \n",
    "                                                  row['k_pts_right_knee_x'], row['k_pts_right_knee_y'])\n",
    "    \n",
    "    distance_nose_wrist_left = distance_keypoints(row['k_pts_nose_x'], row['k_pts_nose_y'], \n",
    "                                                   row['k_pts_left_wrist_x'], row['k_pts_left_wrist_y'])\n",
    "    \n",
    "    distance_nose_wrist_right = distance_keypoints(row['k_pts_nose_x'], row['k_pts_nose_y'], \n",
    "                                                   row['k_pts_right_wrist_x'], row['k_pts_right_wrist_y'])\n",
    "    \n",
    "    # Calculate angles for different joints for left and right side \n",
    "    angle_elbow_left = angle_keypoints(row['k_pts_left_shoulder_x'], row['k_pts_left_shoulder_y'],\n",
    "                                  row['k_pts_left_elbow_x'], row['k_pts_left_elbow_y'],\n",
    "                                  row['k_pts_left_wrist_x'], row['k_pts_left_wrist_y'])\n",
    "    \n",
    "    angle_elbow_right = angle_keypoints(row['k_pts_right_shoulder_x'], row['k_pts_right_shoulder_y'],\n",
    "                                  row['k_pts_right_elbow_x'], row['k_pts_right_elbow_y'],\n",
    "                                  row['k_pts_right_wrist_x'], row['k_pts_right_wrist_y'])\n",
    "    \n",
    "    \n",
    "    angle_knee_left = angle_keypoints(row['k_pts_left_hip_x'], row['k_pts_left_hip_y'],\n",
    "                                      row['k_pts_left_knee_x'], row['k_pts_left_knee_y'],\n",
    "                                      row['k_pts_left_ankle_x'], row['k_pts_left_ankle_y'])\n",
    "    \n",
    "    angle_knee_right = angle_keypoints(row['k_pts_right_hip_x'], row['k_pts_right_hip_y'],\n",
    "                                       row['k_pts_right_knee_x'], row['k_pts_right_knee_y'], \n",
    "                                       row['k_pts_right_ankle_x'], row['k_pts_right_ankle_y'])\n",
    "    \n",
    "    angle_hip_right = angle_keypoints(row['k_pts_right_shoulder_x'], row['k_pts_right_shoulder_y'], \n",
    "                                      row['k_pts_right_hip_x'], row['k_pts_right_hip_y'], \n",
    "                                      row['k_pts_right_knee_x'], row['k_pts_right_knee_y'])\n",
    "    \n",
    "    angle_hip_left = angle_keypoints(row['k_pts_left_shoulder_x'], row['k_pts_left_shoulder_y'], \n",
    "                                     row['k_pts_left_hip_x'], row['k_pts_left_hip_y'], \n",
    "                                     row['k_pts_left_knee_x'], row['k_pts_left_knee_y'])\n",
    "    \n",
    "    # Keeping track of the head during the activites \n",
    "    head_position_x = row['k_pts_nose_x']\n",
    "    head_position_y = row['k_pts_nose_y']\n",
    "    eye_left_x = row['k_pts_left_eye_x']\n",
    "    eye_left_y = row['k_pts_left_eye_y']\n",
    "    eye_right_x = row['k_pts_right_eye_x']\n",
    "    eye_right_y = row['k_pts_right_eye_y']\n",
    "    \n",
    "    # Extra features \n",
    "    feature1 = row['k_pts_left_shoulder_y'] ** 2 - row['k_pts_right_ankle_x']\n",
    "    feature2 = np.arctan2(row['k_pts_left_hip_x'] - row['k_pts_right_elbow_y'], row['k_pts_right_shoulder_y'] - row['k_pts_left_knee_x'])\n",
    "    feature3 = row['k_pts_right_knee_x'] * row['k_pts_left_ankle_y']\n",
    "    feature4 = np.exp(row['k_pts_right_elbow_x'] - row['k_pts_left_knee_y'])\n",
    "    feature5 = np.log(abs(row['k_pts_left_wrist_x'] - row['k_pts_right_ankle_y']) + 1)\n",
    "      \n",
    "    # Append the calculated features for each row into the list\n",
    "    features.append({\n",
    "            'Video': row['Video'],\n",
    "            'Frame': row['frame_number'],\n",
    "            'activity_label': row['activity_label'],\n",
    "            'distance_wrist_shoulder_left' : distance_wrist_shoulder_left,\n",
    "            'distance_wrist_shoulder_right' : distance_wrist_shoulder_right,\n",
    "            'distance_hip_ankle_left' : distance_hip_ankle_left,\n",
    "            'distance_hip_ankle_right' : distance_hip_ankle_right,\n",
    "            'distance_nose_ankle_left' : distance_nose_ankle_left,\n",
    "            'distance_nose_ankle_right' : distance_nose_ankle_right,\n",
    "            'distance_nose_knee_left' : distance_nose_knee_left,\n",
    "            'distance_nose_knee_right' : distance_nose_knee_right,\n",
    "            'distance_nose_wrist' : distance_nose_wrist_left,\n",
    "            'distance_nose_wrist_right' : distance_nose_wrist_right,\n",
    "            'head_position_x' : head_position_x,\n",
    "            'head_position_y' : head_position_y,\n",
    "            'eye_left_x': eye_left_x,\n",
    "            'eye_left_y': eye_left_y,\n",
    "            'eye_right_x': eye_right_x,\n",
    "            'eye_right_y': eye_right_y,\n",
    "            'feature1': feature1,\n",
    "            'feature2': feature2,\n",
    "            'feature3': feature3,\n",
    "            'feature4': feature4,\n",
    "            'feature5': feature5,\n",
    "            'angle_elbow_left': angle_elbow_left,\n",
    "            'angle_elbow_right' : angle_elbow_right,\n",
    "            'angle_knee_left' : angle_knee_left,\n",
    "            'angle_knee_right' : angle_knee_right,\n",
    "            'angle_hip_right' : angle_hip_right,\n",
    "            'angle_hip_left' : angle_hip_left\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the list of calculated features, and stores it in the processes/preprocess_data folder\n",
    "df_calculated_features = pd.DataFrame(features)\n",
    "df_features = df_calculated_features.copy()\n",
    "df_features.to_csv('processes/preprocess_data/CalculatedFeaturesData.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_task2 = df_features.drop(['activity_label', 'Video', 'Frame'], axis=1) # features \n",
    "y_task2 = df_features['activity_label'] # labels \n",
    "\n",
    "# Split the dataset into training and testing sets (70% for training, 30% for testing)\n",
    "X_train_task2, X_test_task2, y_train_task2, y_test_task2 = train_test_split(X_task2, y_task2, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()), (&#x27;svc&#x27;, SVC(C=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()), (&#x27;svc&#x27;, SVC(C=10))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=10)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(C=10))])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model_task2 = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=10))\n",
    "svm_model_task2.fit(X_train_task2, y_train_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.83      0.85     10221\n",
      "           2       0.95      0.96      0.96     10534\n",
      "           3       0.95      0.95      0.95      6565\n",
      "           4       0.93      0.92      0.93     11793\n",
      "           5       0.89      0.93      0.91      8710\n",
      "           6       0.84      0.89      0.86     12815\n",
      "           7       0.80      0.80      0.80      6971\n",
      "           8       0.85      0.76      0.80     10263\n",
      "           9       0.93      0.93      0.93     13402\n",
      "          10       0.95      0.96      0.95     11301\n",
      "\n",
      "    accuracy                           0.90    102575\n",
      "   macro avg       0.90      0.89      0.89    102575\n",
      "weighted avg       0.90      0.90      0.90    102575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_task2 = svm_model_task2.predict(X_test_task2)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_task2 = accuracy_score(y_test_task2, y_pred_task2)\n",
    "print(f\"Accuracy: {accuracy_task2:.2f}\")\n",
    "\n",
    "# Detailed performance report\n",
    "print(classification_report(y_test_task2, y_pred_task2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: MoViNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoViNet model is a tensorflow model used to predict the activity of human movements. Data from 600 types of activites are collected, including some labels which we have no corresponding video data for (e.g., belly dancing, streching arm, zumba). Ive divided task 3 into two seperate ways to use the MoViNet model for prediction. For both of the tasks im only using one video for armraise (000000.mp4)\n",
    "\n",
    "All the code can be found here for the first task: https://www.tensorflow.org/hub/tutorials/movinet \n",
    "\n",
    "All the code can be found here for the second task: https://github.com/PranayLendave/video_classification/blob/main/trained_movinets_notebook.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle_task3 = \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3\"\n",
    "model_task3 = hub.load(model_handle_task3)\n",
    "sig = model_task3.signatures['serving_default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig.pretty_printed_signature())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710717359.449051 17024035 service.cc:145] XLA service 0x3066ca830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1710717359.449648 17024035 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1710717359.588260 17024035 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "my_labels = ['pushup', 'curl', 'fly', 'squat', 'birddog', 'superman', 'bicyclecrunch', 'legraise', 'armraise', 'overheadpress']\n",
    "\n",
    "video_path = 'raw_data/infinityai_fitness_basic_armraise_v1.0/armraise/000000.mp4'\n",
    "frames = []\n",
    "cap_task3 = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap_task3.isOpened():\n",
    "    \n",
    "    ret, frame = cap_task3.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = tf.image.resize(frame, [224, 224])\n",
    "    frame = tf.cast(frame, tf.float32) / 255.\n",
    "    #frame = tf.expand_dims(frame, axis=0)\n",
    "    frames.append(frame)\n",
    "    \n",
    "cap_task3.release()\n",
    "\n",
    "frames_stack = tf.stack(frames)\n",
    "frames_batch = frames_stack[tf.newaxis, ...]\n",
    "\n",
    "logits = sig(image = frames_batch)\n",
    "logits = logits['classifier_head'][0]\n",
    "print(logits)\n",
    "#sig(image = frames_stack[tf.newaxis, :1])\n",
    "#frames_batch = tf.stack(frames)\n",
    "#print(frames)\n",
    "#print(sig(image = frames_batch[tf.newaxis, ...]))\n",
    "#logits = sig(image = frames_batch[tf.newaxis, ...])\n",
    "#logits = logits['classifier_head'][0]\n",
    "\n",
    "\n",
    "#predictions = model_task3(frames_batch)\n",
    "#predicted_label = my_labels[np.argmax(predictions)]\n",
    "#print(predicted_label)\n",
    "#clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig(image = frames_stack[tf.newaxis, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (3 total):\n    * <tf.Tensor 'inputs:0' shape=(1, 578, 224, 224, 3) dtype=float32>\n    * False\n    * None\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='image')}\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='inputs/image')}\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='inputs/image')}\n    * True\n    * None\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='image')}\n    * True\n    * None\n  Keyword arguments: {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/arinrahim/Desktop/SKOLAN/ML/Assignments/Final assignment/arin_rahim_finalAssignment.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arinrahim/Desktop/SKOLAN/ML/Assignments/Final%20assignment/arin_rahim_finalAssignment.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m model_task3(frames_batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arinrahim/Desktop/SKOLAN/ML/Assignments/Final%20assignment/arin_rahim_finalAssignment.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m predicted_label \u001b[39m=\u001b[39m my_labels[np\u001b[39m.\u001b[39margmax(predictions)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arinrahim/Desktop/SKOLAN/ML/Assignments/Final%20assignment/arin_rahim_finalAssignment.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted_label)\n",
      "File \u001b[0;32m~/Desktop/SKOLAN/ML/Assignments/Final assignment/envFinalAssignment/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:817\u001b[0m, in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_attribute\u001b[39m(instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 817\u001b[0m   \u001b[39mreturn\u001b[39;00m instance\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/SKOLAN/ML/Assignments/Final assignment/envFinalAssignment/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/SKOLAN/ML/Assignments/Final assignment/envFinalAssignment/lib/python3.10/site-packages/tensorflow/python/saved_model/function_deserialization.py:335\u001b[0m, in \u001b[0;36mrecreate_function.<locals>.restored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m   positional, keyword \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mstructured_input_signature\n\u001b[1;32m    332\u001b[0m   signature_descriptions\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    333\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mOption \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  Keyword arguments: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    334\u001b[0m           index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, _pretty_format_positional(positional), keyword))\n\u001b[0;32m--> 335\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCould not find matching concrete function to call loaded from the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSavedModel. Got:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m{\u001b[39;00m_pretty_format_positional(args)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  Keyword \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marguments: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m Expected these arguments to match one of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfollowing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(saved_function\u001b[39m.\u001b[39mconcrete_functions)\u001b[39m}\u001b[39;00m\u001b[39m option(s):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39mchr\u001b[39m(\u001b[39m10\u001b[39m)\u001b[39m+\u001b[39m\u001b[39mchr\u001b[39m(\u001b[39m10\u001b[39m))\u001b[39m.\u001b[39mjoin(signature_descriptions)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (3 total):\n    * <tf.Tensor 'inputs:0' shape=(1, 578, 224, 224, 3) dtype=float32>\n    * False\n    * None\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='image')}\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='inputs/image')}\n    * False\n    * None\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='inputs/image')}\n    * True\n    * None\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (3 total):\n    * {'image': TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name='image')}\n    * True\n    * None\n  Keyword arguments: {}"
     ]
    }
   ],
   "source": [
    "predictions = model_task3(frames_batch)\n",
    "predicted_label = my_labels[np.argmax(predictions)]\n",
    "print(predicted_label)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to set up some helper code.\n",
    "\n",
    "# Download Kinetics 600 label map\n",
    "!wget https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt -O labels.txt -q\n",
    "\n",
    "with tf.io.gfile.GFile('labels.txt') as f:\n",
    "  lines = f.readlines()\n",
    "  KINETICS_600_LABELS_LIST = [line.strip() for line in lines]\n",
    "  KINETICS_600_LABELS = tf.constant(KINETICS_600_LABELS_LIST)\n",
    "\n",
    "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
    "  \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\"\n",
    "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
    "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
    "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
    "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
    "  return tuple(zip(top_labels, top_probs))\n",
    "\n",
    "def predict_top_k(model, video, k=5, label_map=KINETICS_600_LABELS):\n",
    "  \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\"\n",
    "  outputs = model.predict(video[tf.newaxis])[0]\n",
    "  probs = tf.nn.softmax(outputs)\n",
    "  return get_top_k(probs, k=k, label_map=label_map)\n",
    "\n",
    "def load_movinet_from_hub(model_id, model_mode, hub_version=3):\n",
    "  \"\"\"Loads a MoViNet model from TF Hub.\"\"\"\n",
    "  hub_url = f'https://tfhub.dev/tensorflow/movinet/{model_id}/{model_mode}/kinetics-600/classification/{hub_version}'\n",
    "\n",
    "  encoder = hub.KerasLayer(hub_url, trainable=True)\n",
    "\n",
    "  inputs = tf.keras.layers.Input(\n",
    "      shape=[None, None, None, 3],\n",
    "      dtype=tf.float32)\n",
    "\n",
    "  if model_mode == 'base':\n",
    "    inputs = dict(image=inputs)\n",
    "  else:\n",
    "    # Define the state inputs, which is a dict that maps state names to tensors.\n",
    "    init_states_fn = encoder.resolved_object.signatures['init_states']\n",
    "    state_shapes = {\n",
    "        name: ([s if s > 0 else None for s in state.shape], state.dtype)\n",
    "        for name, state in init_states_fn(tf.constant([0, 0, 0, 0, 3])).items()\n",
    "    }\n",
    "    states_input = {\n",
    "        name: tf.keras.Input(shape[1:], dtype=dtype, name=name)\n",
    "        for name, (shape, dtype) in state_shapes.items()\n",
    "    }\n",
    "\n",
    "    # The inputs to the model are the states and the video\n",
    "    inputs = {**states_input, 'image': inputs}\n",
    "\n",
    "  # Output shape: [batch_size, 600]\n",
    "  outputs = encoder(inputs)\n",
    "\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.build([1, 1, 1, 1, 3])\n",
    "\n",
    "  return model\n",
    "\n",
    "# Download example gif\n",
    "!wget https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif -O jumpingjack.gif -q\n",
    "\n",
    "def load_gif(file_path, image_size=(224, 224)):\n",
    "  \"\"\"Loads a gif file into a TF tensor.\"\"\"\n",
    "  with tf.io.gfile.GFile(file_path, 'rb') as f:\n",
    "    video = tf.io.decode_gif(f.read())\n",
    "  video = tf.image.resize(video, image_size)\n",
    "  video = tf.cast(video, tf.float32) / 255.\n",
    "  return video\n",
    "\n",
    "def get_top_k_streaming_labels(probs, k=5, label_map=KINETICS_600_LABELS_LIST):\n",
    "  \"\"\"Returns the top-k labels over an entire video sequence.\n",
    "\n",
    "  Args:\n",
    "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
    "      the probability of each class on each frame.\n",
    "    k: the number of top predictions to select.\n",
    "    label_map: a list of labels to map logit indices to label strings.\n",
    "\n",
    "  Returns:\n",
    "    a tuple of the top-k probabilities, labels, and logit indices\n",
    "  \"\"\"\n",
    "  top_categories_last = tf.argsort(probs, -1, 'DESCENDING')[-1, :1]\n",
    "  categories = tf.argsort(probs, -1, 'DESCENDING')[:, :k]\n",
    "  categories = tf.reshape(categories, [-1])\n",
    "\n",
    "  counts = sorted([\n",
    "      (i.numpy(), tf.reduce_sum(tf.cast(categories == i, tf.int32)).numpy())\n",
    "      for i in tf.unique(categories)[0]\n",
    "  ], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_probs_idx = tf.constant([i for i, _ in counts[:k]])\n",
    "  top_probs_idx = tf.concat([top_categories_last, top_probs_idx], 0)\n",
    "  top_probs_idx = tf.unique(top_probs_idx)[0][:k+1]\n",
    "\n",
    "  top_probs = tf.gather(probs, top_probs_idx, axis=-1)\n",
    "  top_probs = tf.transpose(top_probs, perm=(1, 0))\n",
    "  top_labels = tf.gather(label_map, top_probs_idx, axis=0)\n",
    "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
    "\n",
    "  return top_probs, top_labels, top_probs_idx\n",
    "\n",
    "def plot_streaming_top_preds_at_step(\n",
    "    top_probs,\n",
    "    top_labels,\n",
    "    step=None,\n",
    "    image=None,\n",
    "    legend_loc='lower left',\n",
    "    duration_seconds=10,\n",
    "    figure_height=500,\n",
    "    playhead_scale=0.8,\n",
    "    grid_alpha=0.3):\n",
    "  \"\"\"Generates a plot of the top video model predictions at a given time step.\n",
    "\n",
    "  Args:\n",
    "    top_probs: a tensor of shape (k, num_frames) representing the top-k\n",
    "      probabilities over all frames.\n",
    "    top_labels: a list of length k that represents the top-k label strings.\n",
    "    step: the current time step in the range [0, num_frames].\n",
    "    image: the image frame to display at the current time step.\n",
    "    legend_loc: the placement location of the legend.\n",
    "    duration_seconds: the total duration of the video.\n",
    "    figure_height: the output figure height.\n",
    "    playhead_scale: scale value for the playhead.\n",
    "    grid_alpha: alpha value for the gridlines.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the output numpy image, figure, and axes.\n",
    "  \"\"\"\n",
    "  num_labels, num_frames = top_probs.shape\n",
    "  if step is None:\n",
    "    step = num_frames\n",
    "\n",
    "  fig = plt.figure(figsize=(6.5, 7), dpi=300)\n",
    "  gs = mpl.gridspec.GridSpec(8, 1)\n",
    "  ax2 = plt.subplot(gs[:-3, :])\n",
    "  ax = plt.subplot(gs[-3:, :])\n",
    "\n",
    "  if image is not None:\n",
    "    ax2.imshow(image, interpolation='nearest')\n",
    "    ax2.axis('off')\n",
    "\n",
    "  preview_line_x = tf.linspace(0., duration_seconds, num_frames)\n",
    "  preview_line_y = top_probs\n",
    "\n",
    "  line_x = preview_line_x[:step+1]\n",
    "  line_y = preview_line_y[:, :step+1]\n",
    "\n",
    "  for i in range(num_labels):\n",
    "    ax.plot(preview_line_x, preview_line_y[i], label=None, linewidth='1.5',\n",
    "            linestyle=':', color='gray')\n",
    "    ax.plot(line_x, line_y[i], label=top_labels[i], linewidth='2.0')\n",
    "\n",
    "\n",
    "  ax.grid(which='major', linestyle=':', linewidth='1.0', alpha=grid_alpha)\n",
    "  ax.grid(which='minor', linestyle=':', linewidth='0.5', alpha=grid_alpha)\n",
    "\n",
    "  min_height = tf.reduce_min(top_probs) * playhead_scale\n",
    "  max_height = tf.reduce_max(top_probs)\n",
    "  ax.vlines(preview_line_x[step], min_height, max_height, colors='red')\n",
    "  ax.scatter(preview_line_x[step], max_height, color='red')\n",
    "\n",
    "  ax.legend(loc=legend_loc)\n",
    "\n",
    "  plt.xlim(0, duration_seconds)\n",
    "  plt.ylabel('Probability')\n",
    "  plt.xlabel('Time (s)')\n",
    "  plt.yscale('log')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  fig.canvas.draw()\n",
    "\n",
    "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close()\n",
    "\n",
    "  figure_width = int(figure_height * data.shape[1] / data.shape[0])\n",
    "  image = PIL.Image.fromarray(data).resize([figure_width, figure_height])\n",
    "  image = np.array(image)\n",
    "\n",
    "  return image, (fig, ax, ax2)\n",
    "\n",
    "def plot_streaming_top_preds(\n",
    "    probs,\n",
    "    video,\n",
    "    top_k=5,\n",
    "    video_fps=25.,\n",
    "    figure_height=500,\n",
    "    use_progbar=True):\n",
    "  \"\"\"Generates a video plot of the top video model predictions.\n",
    "\n",
    "  Args:\n",
    "    probs: probability tensor of shape (num_frames, num_classes) that represents\n",
    "      the probability of each class on each frame.\n",
    "    video: the video to display in the plot.\n",
    "    top_k: the number of top predictions to select.\n",
    "    video_fps: the input video fps.\n",
    "    figure_fps: the output video fps.\n",
    "    figure_height: the height of the output video.\n",
    "    use_progbar: display a progress bar.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array representing the output video.\n",
    "  \"\"\"\n",
    "  video_fps = 8.\n",
    "  figure_height = 500\n",
    "  steps = video.shape[0]\n",
    "  duration = steps / video_fps\n",
    "\n",
    "  top_probs, top_labels, _ = get_top_k_streaming_labels(probs, k=top_k)\n",
    "\n",
    "  images = []\n",
    "  step_generator = tqdm.trange(steps) if use_progbar else range(steps)\n",
    "  for i in step_generator:\n",
    "    image, _ = plot_streaming_top_preds_at_step(\n",
    "        top_probs=top_probs,\n",
    "        top_labels=top_labels,\n",
    "        step=i,\n",
    "        image=video[i],\n",
    "        duration_seconds=duration,\n",
    "        figure_height=figure_height,\n",
    "    )\n",
    "    images.append(image)\n",
    "\n",
    "  return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_movinet_from_hub('a2', 'base', hub_version=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "The extraction of keypoints from the videos for different activities went as expected. The MoveNet model was used to detect human poses and extract keypoints along with their respective confidence values for each frame of every video. The data extraction took time, but having an organized dataset makes training and fitting a model easier. As mentioned earlier, errors prevented me from using an LSTM, and an SVM model was used instead. The SVM model performed well and reported an accuracy of 94%. I'm satisfied with the accuracy, but since the dataset is considered large and avoiding overfitting with an LSTM can be done by using early stopping, I would have preferred using a neural network over the SVM model to have a more robust model with the possibility to manage overfitting scenarios. Down below are the classification report for the SVM model: \n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.92      0.90      0.91     10221\n",
    "           2       0.98      0.99      0.98     10534\n",
    "           3       0.98      0.98      0.98      6565\n",
    "           4       0.98      0.97      0.97     11793\n",
    "           5       0.94      0.96      0.95      8710\n",
    "           6       0.89      0.94      0.91     12815\n",
    "           7       0.88      0.88      0.88      6971\n",
    "           8       0.90      0.85      0.87     10263\n",
    "           9       0.97      0.98      0.98     13402\n",
    "          10       0.98      0.99      0.98     11301\n",
    "\n",
    "    accuracy                           0.94    102575\n",
    "   \n",
    "   macro avg       0.94      0.94      0.94    102575\n",
    "\n",
    "weighted avg       0.94      0.94      0.94    102575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Task 1 gave us a total of 51 features to use for our machine learning model. However, using  all these features at once may increases the risk of having a feature that is too dominangt, and lead to overfitting the model. Feature engineering is used to decrease the number of features that is used for our model. By using less features we decrease both the computional load and the risk of overfitting. \n",
    "\n",
    "Since we are trying to predict activity, I used the distance between the different joints and the angle between different joints. Calculating the distance and angle is a way to be able to tell the labels apart. For example in armraise, the angle of the elbow and the distance between the wrist and shoulder decreases, while the angle of the knee and the distance between the hip and ankle is consistent. For a different activity such as squats, the angle of the knee and the distanece between the nose and ankle both decreases. Lastly, I added some more feature that keeps track of the head during the different activites, and some other extra helpful features. \n",
    "\n",
    "I had the same problem as in task 1, neural network did not work so SVM model was used again. The data was scaled using StandarScaler, the kernal is used was Radial Basis Function (rbf) with a regularization parameter of 10. A total of 27 features were engineered, which is 24 fewer than in task 1. The model reported an accuracy of 90 %. Although I got a 4 % decrease in model prediction accuracy compared to task 1, considering the reduced number of features, I wouldnt state that the overall performance was significantly affected. Down below are the classification report for the SVM model: \n",
    "\n",
    "\n",
    "Minimal set of features: 27. \n",
    "\n",
    "\n",
    "Accuracy: 0.90\n",
    "              \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.87      0.83      0.85     10221\n",
    "           2       0.95      0.96      0.96     10534\n",
    "           3       0.95      0.95      0.95      6565\n",
    "           4       0.93      0.92      0.93     11793\n",
    "           5       0.89      0.93      0.91      8710\n",
    "           6       0.84      0.89      0.86     12815\n",
    "           7       0.80      0.80      0.80      6971\n",
    "           8       0.85      0.76      0.80     10263\n",
    "           9       0.93      0.93      0.93     13402\n",
    "          10       0.95      0.96      0.95     11301\n",
    "\n",
    "    accuracy                           0.90    102575\n",
    "   \n",
    "   \n",
    "   macro avg       0.90      0.89      0.89    102575\n",
    "\n",
    "\n",
    "weighted avg       0.90      0.90      0.90    102575\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "The MoViNet model is trained on a big dataset called Kinetics 600, which includes 600 different types of activities. Some of these activities involve moving parts of the body like arms and legs, bending arms, or moving wrists. Even though we didn't specifically teach the model to recognize our exact activities, it can still guess with some accuracy. For example, if our video shows someone raising their arm, the model might guess the person is doing an activity that involves arm raising. The guess might not be exactly right, but it shows the model knows which body parts are moving.\n",
    "\n",
    "I tried to use the MoViNet model for our project, but I ran into some problems. I looked online for help on sites like Stack Overflow and GitHub but couldn't fix the issues. After spending a lot of time trying to debug without any luck, I decided to use Google Colab instead, because TensorFlow has a special link for it. You can see the results of that below:\n",
    "\n",
    "tai chi             : 0.331\n",
    "\n",
    "belly dancing       : 0.298\n",
    "\n",
    "stretching arm      : 0.183\n",
    "\n",
    "yoga                : 0.051\n",
    "\n",
    "doing aerobics      : 0.014\n",
    "\n",
    "The outcome wasn't quite what I expected because there's an activity called \"front raises\" in the Kinetics 600 dataset. I thought the model would guess something close to \"front raises\" for videos where someone is lifting their arms. However, it guessed activities like \"tai chi\" or \"belly dancing\" instead, which also have a lot of arm movements.\n",
    "\n",
    "\n",
    "Link to the google colab: https://colab.research.google.com/drive/1MU3gp33guZEFeVzRvYBXWZPi6Dzrr66n?usp=sharing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (envFinalAssignment)",
   "language": "python",
   "name": "envfinalassignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
